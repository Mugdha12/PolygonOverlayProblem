\documentclass{article}
\usepackage{Sweave}
\begin{document}
<<>>=
knitr::opts_chunk$set(cache = TRUE, echo=FALSE, warning  = FALSE, message = FALSE)
@

<<LoadingLibraries>>=
library(areal)
library(dplyr)   # data wrangling
library(sf) 
library(rgdal)
library(lwgeom)
library(readr)
library(curl)
library(ggplot2)
library(sp)
library(gstat)
setwd("/Users/mugdhachowdary/Desktop/OneDrive - University of St. Thomas/STAT/STAT 460/STAT460")
@
\section{Background}
\subsection{Modifiable Areal Unit Problem (MAUP)}
The modifiable areal unit problem (MAUP) is “a problem arising from the imposition of artificial units of spatial reporting on continuous geographical phenomena resulting in the generation of artificial spatial patterns”. Viewing a map with millions of dots does no give us particularly useful information to analyze how certain
variable varies its value among the space. Thus, the choropleth map is commonly used for the purpose of analyzing spatial data. It is a thematic map that is divided into several polygons and each of the polygons is shaded in proportion to the value of the statistical variable that we are interested in. By dividing the map
into different areas and observing how dense data dots are grouped in each are, we could visualize how the
variable is associated to the spatial information. Take the cancer rate in a population as an example. “A
map with millions of dots indicating cancer diagnosis is not particularly useful, but a map showing different
cancer rates per population by county may be more so.”. However, although grouping data dots by
area may show the association between the response variable and the space, different grouping scheme may
lead to different visualization of the data pattern, which is MAUP.
\subsection{Flint Water crisis}
The Flint Water Crisis is an ongoing public health concern occurring in Flint, Michigan. This issue began in
April 2014 when Flint changed their water supply to the Flint River. The water was treated with chemicals
to remove any contaminants in order to make the water safe for use and consumption. However, inadequate
2
water treatment and monitoring caused the water pipes to corrode and the lead from the pipes leaked into
the water. This water was consumed by people residing in Flint and became a health concern once there
were reports on lead being present in their blood.
Lead is a chemical, specifically a type of natural metal, that may take weeks (or not at all) to leave the body
depending where it is stored in the body. High levels of lead may result in lead poisoning causing brain and
kidney damage, and in severe cases, death. Children are more susceptible than adults to the adverse health
problems caused by lead during their developmental stages since their physical, behavioral, and cognitive
health are at risk. Currently, there is no specified safe level of lead within children.
In order to verify as to whether the lead in the new water supply posed as a health risk to the people of Flint,
officials directed health professionals to collect blood samples from individuals living within the ZIP codes
of Flint so to measure the lead levels within each sample. If there was an increase in the number of Flint
residents with lead in their blood or if lead levels increased significantly enough to be a health risk as a result
from consuming the water, then the water supply from the Flint River would prove as a concern for further
investigation. Officials reported that the blood lead levels remained steady before and after the change in
Flint’s water supply, that is, the lead levels in blood samples obtained after the water supply changed to the
river did not significantly differ from the blood lead levels due to the previous water supply .
Hence, it seemed as if the change in the water supply was not a concern during that time.
However, Dr. Mona Hanna-Attisha, a director at one of the pediatric units in Flint, Michigan, was not
convinced by such results since they were not consistent with the results from her study of analyzing the
lead levels in infants and children of ages 5 and younger. She reported in September 2015, that at least
"twice as many children had elevated levels of lead in their blood" since Flint’s water supply changed in
2014 . She reached out to her collaborator, Richard Sadler, a geographer at Michigan State
University, and requested for him to analyze the lead levels within the blood to readdress whether the
current Flint water supply would be unsafe for use and consumption. Specifically, they conducted a study
determining whether there was a significant increase in the number of children with elevated lead levels.
Sadler analyzed data that came from blood samples obtained from a local children’s hospital and he applied
spatial analysis at the neighborhood-level; that is, he analyzed data from children who lived within the
neighborhoods of Flint city. Contrary to what the officials reported, Sadler and his collaborators found that
found that there was a significant increase in the number of children with elevated levels of lead since the
water supply has changed.
Flint
\section{Methods}
\subsection{Data}
In this project, we primarily worked with three different datasets, Flint, BLLZIPCODE and Sfzipcodegrid.Using these datasets we want to find percent of the lead in population under 6 and compare them at Flint city and zipcode  levels. 
Flint dataset was originally extracted from city shapefile that is imported from from Michigan state department boundaries. Since, we only wanted to focus on Flint we have filtered the shapefile using the city name "Flint".  The resulting dataset contains the following primary variables:
\begin{itemize}
\item ObjectID: A unique identifer of the current city.
\item FIPSCODE: A code to uniquely identify geographic areas.
\item Name: The name of the city.
\item Type: It specifies the type of the object (whether it is a city or county).
\item geometry: This variables gives us the city's boundaries/shape in the form of polygon outline.
\end{itemize}
Like Flint dataset, BLLZIPCODE dataset was also extracted from a shapefile called "BLL" that contains the data of Michigan Childhood Blood Lead Levels data reported by The Michigan Department of Health and Human Resources. This csv file was imported from Dr.McNamara’s Github repository titled "BLL\/under6\/zip\/2016", a set showing results for blood tests done on children under the age of 6, sorted by zipcode, contains 935 rows with 6 variables. Since, our primary focus is Flint,  we only selected rows whose zipcodes are covered by Flint and ended up storing the result in BLLflint. Since,BLLflint is a dataframe to convert this into shapefile, we performed left join on BLLflint and FlintZP, which resulted in the BLLZIPCODE sf object.  BLLZIPCODE contains the following primary variables:
\begin{itemize}
\item ZCTA5CE10: Unique identifer for the area.
\item Pop\_under\_age\_six: The total count of the population under age 6.
\item any\_samples\_percent:The percent of children with BLL greater than five micrograms of lead per decilitor of blood.
\item geometry: This variables gives us the zipcode's boundaries\/shape in the form of list.      
\end{itemize}
Sfzipcodegrid(grid) is an extension of BLLZIPCODE(sfobject). While BLLZIPCODE only contains 7 out of 12 zipcodes of Flint, Sfzipcodegrid contains all the 12 zipcodes. This grid is made from the FlintZp code datafile that contains the geometric data of all the zipcodes in Flint.  The number of values grid contains would be equivalent to the specified cell size of the grid, and it only contains the variables geometric attribute of each cell,  along with it's id.

\subsubsection{Data Wrangling:}
After extracting and cleaning the datasets, we performed by data wrangling by converting BLLZIPCODE and Flint to sf object. In addition to this, we also made sure that all of the datasets are using the same coordinate system to ensure that we don't get any errors when validating our dataset in the process of performing areal interpolation.

\subsection{R packages}
Functions from R packages such as "areal", "tidyverse", "sf", "rgdal", "curl", "sp" and "gstat" were used.
\subsubsection{areal}
This package aims to reduce the barries that prevent a spatial researcher from performing Areal Interpolation. As its previously stated purpose, this package was mainly used to perform areal interpolation to generate polygon data at city level from polygon aggregrated at zipcode level.  Functions such as ar\_validate and aw\_interpolate were used to perform the interpolation. The function ar\_validate ensures that our data was ready for areal interpolation by checking five conditions. These conditions include: 
\begin{enumerate}
\item Are both objects sf objects?
\item Do both objects share the same coordinate system?
\item Is that coordinate system planar?
\item Do the given variables exist in the source data?
\item Will interpolation overwrite columns in the target data?
\end{enumerate}
The function aw\_interpolate was then used to perform  areal interpolation on the now validated source and target datasets.
\subsubsection{gstat}
This package is used for geostatistical modelling, prediction and simulation. We mainly used this package for variogram modelling, plotting and kriging. Function gstat::vgm generates a variogram model taking the lattice layers as input. We then used function gstat::variogram to calculates the sample variogram from data. After storing our variogram object, we utilized gstats::fit.variogram function to fit a simple or nested variogram model to a sample variogram. As a result, we could call function krige0 which is a user-supplied covariance function for simple, ordinary or universal kriging.
\subsubsection{sf}
This package supports for simple features, a standardized way to encode spatial vector data. Binds to 'GDAL' for reading and writing data, to 'GEOS' for geometrical operations, and to 'PROJ' for projection conversions and datum transformations. All of the packages, this was the most used package in our project as we are dealing with shapefiles and used functions like st\_read, st\_transform, and st\_as\_sf for data wrangling. The function st\_make\_grid was used to make a grid out of the shapefile.
\subsubsection{sp}
This package provides classes and methods for spatial data; the classes document where the spatial location information resides, for 2D or 3D data. Utility functions are provided, e.g. for plotting data as maps, spatial selection, as well as methods for retrieving coordinates, for subsetting, print, summary, etc. In our project, this package was mainly used when performing krigging as the sf objects should be converted to spdf objects prior to the process.
\subsubsection{tidyverse}
This is a single "meta"-package, which installs a collection of packages with a simple command. In this project, we used this package to install packages like ggplot2, dplyr , readr with one R command library(tidyverse).
\begin{enumerate}
\item ggplot2: This package  helps us to graphically show our resulting polygon data using ggplot, geom\_sf, geom\_polygon and scale\_fill\_gradient functions.
\item dplyr: This package helps us to combine two different datasets using inner\_join command.
\item readr: This package was primarily used to import data from csv files, in this project.
\end{enumerate}



\subsection{Areal Interpolation:}
Areal interpolation is the process making estimates from a source set of polygons to an overlapping but incongruent set of target polygons. This is required if, for example, a researcher wants to derive population estimates for neighborhoods in a U.S. city from the Census Bureau’s census tracts. Neighborhoods do not typically align with census tract boundaries, and areal interpolation can be used to produce estimates in this situation.

\subsection{Kriging:}
Kriging is the process used when there is spatially correlated distance or directional bias in the data. It assumes that the distance or direction between sample points reflects a spatial correlation that can be used to explain variation in the surface. The Kriging tool fits a mathematical function to a specified number of points, or all points within a specified radius, to determine the output value for each location. This method is a multistep process; it includes exploratory statistical analysis of the data, variogram modeling, creating the surface, and (optionally) exploring a variance surface.

\section{Validating Areal Interpolation:}
Before we started our interpolation, we wanted to validate whether areal interpolation is an effective way to explore MAUP. In order to achieve this, we performed interpolation on population aggregrated at congressDist to aggregrate the population at the zipcode level and vice versa. Based on this, we were able to verify Areal Interpolation was one of the valid methods for COSP. Also, we were able to find that the areal interpolation is not pynchophalactic, but the aggregrated values generated by aw\_interpolate are more accurate compared to the values produced st\_interpolate\_aw method of sf object. 

<<ValidatingArealInterpolation>>=
###Aggregating population density by zipcode from population density by congressDist
congressDistrictShapeFile <-sf::st_read("Data/C2012", layer = "C2012", quiet=TRUE)
congressDistrict <- select(congressDistrictShapeFile, ID, AREA,DISTRICT)
zipcodeShapeFile <- sf::st_read("Data/tl_2010_27_zcta510",  layer = "tl_2010_27_zcta510", quiet=TRUE)

nzipcodeShapeFile <- sf::st_read("Data/shp_bdry_zip_code_tabulation_areas",  layer = "zip_code_tabulation_areas", quiet=TRUE)
zipcodeShapeFile <- zipcodeShapeFile[order(zipcodeShapeFile$ZCTA5CE10),]
zipcode <- select(zipcodeShapeFile,STATEFP10, ZCTA5CE10,GEOID10 )
congressDistrictPopulation <- read_csv("Data/DEC_10_SF1_P1_with_ann_CD.csv")
colnames(congressDistrict)[3] <- "CONGRESSDIST"

congressDistrictPopulation$CONGRESSDIST <- congressDistrict$CONGRESSDIST
popByCongress <- dplyr::left_join(congressDistrict, congressDistrictPopulation, by="CONGRESSDIST")
colnames(popByCongress)[5] <- "POPULATION"
zipcodeShape<- st_transform(zipcode, crs = 26915, proj4string = "+proj=longlat +datum=WGS84")
nPopByCongress<-st_transform(popByCongress, crs = 26915, proj4string = "+proj=longlat +datum=WGS84")
ar_validate(source=nPopByCongress, target=zipcodeShape, varList = "DENSITY", method="aw", verbose=TRUE)
result <- aw_interpolate(zipcodeShape, tid="ZCTA5CE10", source = nPopByCongress, sid=CONGRESSDIST, weight = "sum", output = "sf", intensive = "DENSITY")
#g <- st_make_grid(zipcodeShape, cellsize = 10000)
#g <- st_as_sf(g)
#resultST <- sf::st_interpolate_aw(nPopByCongress["DENSITY"], g, extensive=FALSE)

###Aggregating population density by congressDist  from population density by zipcode

zipcodePopulation <- read_csv("Data/DEC_10_SF1_P1_with_ann.csv")
zipcode <- select(nzipcodeShapeFile, "ZCTA5CE10" , "Shape_Area")
zipcodeN <- zipcode[order(zipcode$ZCTA5CE10),] 
colnames(zipcodeN)[1] <- "ZCTA"
zipcodePopulation$ZCTA <- zipcodeN$ZCTA
congressDist <- select(nPopByCongress, "ID",           "AREA",         "CONGRESSDIST")
popByZipcode <- dplyr::left_join(zipcodeN,zipcodePopulation,  by="ZCTA")
popByZipcode["AreaSQMILE"] <- popByZipcode$Shape_Area * (3.861/10000000)
popByZipcode["DENSITY"] <- popByZipcode$POPULATION/popByZipcode$AreaSQMILE

npopByZipcode <- st_transform(popByZipcode, crs = 26915, proj4string = "+proj=longlat +datum=WGS84")
ar_validate(source=npopByZipcode, target=congressDist, varList = "POPULATION", method="aw", verbose=TRUE)
resultOPP <- aw_interpolate(congressDist, tid= "CONGRESSDIST", source = npopByZipcode, sid="ZCTA", weight = "sum", output = "sf", intensive = "DENSITY")
#resultSTOPP <- sf::st_interpolate_aw(npopByZipcode["DENSITY"], congressDist["CONGRESSDIST"], extensive=FALSE)


#### Difference between the densities
##zipcode
#zpresidual <- npopByZipcode$DENSITY - result$DENSITY
#zpSTresidual <- npopByZipcode$DENSITY - resultST$DENSITY
#zpWIresidual <- result$DENSITY - resultST$DENSITY
#paste("The mean difference between actual density and areal interpolation is: ", mean(zpresidual))
#paste("The mean difference between actual density and st areal interpolation is: ", mean(zpSTresidual))
#paste("The mean difference between areal interpolation and st areal interpolation is: ", mean(zpWIresidual))

##CongressionalDistricts
#cdresidual <- nPopByCongress$DENSITY - resultOPP$DENSITY
#cdSTresidual <- nPopByCongress$DENSITY - resultSTOPP$DENSITY
#cdWIresidual <- resultOPP$DENSITY - resultSTOPP$DENSITY
#paste("The mean difference between actual density and areal interpolation is: ", mean(cdresidual))
#paste("The mean difference between actual density and st areal interpolation is: ", mean(cdSTresidual))
#paste("The mean difference between areal interpolation and st areal interpolation is: ", mean(cdWIresidual))
@

\section{Exploring Modifiable Areal Unit Problem using Flint water crisis data:}
After making sure, Areal Interpolation is one of the possible ways to explore MAUP. We went ahead and started our process of performing areal interpolation. As explained in the data section, we have done data cleaning and data wrangling to ensure we can do interpolation without any interruptions. We wanted perform interpolation twice, since we wanted to aggregrate BLL values from zipcode level to the city level and another from few zipcodes to all the zipcodes. The main purpose of the second interpolation is due to the fact that we dont have data for 5 out of 12 zipcodes in Flint. By performing this interpolation, we want to aggregrate the missing data for these 5 zipcodes.  Before performing interpolation, we validated our datasets are ready for interpolation by using the function ar\_validate on the source(BLLZIPCODE) and target(Sfzipcodegrid and flint) datasets. Later, we used the function ar\_interpolation on the source and target datasets, which resulted in the two interpolated datasets. The visualizations of this datasets can be seen below. When we interpolated from zipcode level to city level, we got a value of 3.4211, which is smaller than 5mg\/dl. 

<<ExploringMAUPusingFlintwaterdatawitharealinterpolation>>=
###Importing flint city shape file
city_shapefile <-sf::st_read("Data/Cities_v17a", layer =  "Cities_v17a", quiet=TRUE)
FlintCity <- city_shapefile %>% filter(NAME == "Flint")
flint<- st_transform(FlintCity, crs = 26915, proj4string = "+proj=longlat +datum=WGS84")

###Importing Michigan zipcodes
zipcode <- sf::st_read("Data/cb_2015_us_zcta510_500k",layer="cb_2015_us_zcta510_500k", quiet=TRUE)
zipcode<-st_transform(zipcode, crs = 26915, proj4string = "+proj=longlat +datum=WGS84")
###Sort out the zipcode around flint river area 

zip <- c('48501','48502', '48503', '48504','48505', '48506', '48507','48509', '48519', '48529', '48531', '48532', '48550', '48551','48552', '48553', '48554','48555', '48556', '48557')
FlintZP <- zipcode %>% subset(ZCTA5CE10 %in% zip)
FlintZP[is.na(FlintZP)] <- 0
plot(FlintZP)

###Importing BLL data

BLL <- read_csv(curl("https://raw.githubusercontent.com/AmeliaMN/BLL/master/2015/BLL_under6_zip_2015.csv"), na = c("NA", " ", "**"))

colnames(BLL)[1] <- "ZCTA5CE10" 
BLL$ZCTA5CE10 <- as.factor(BLL$ZCTA5CE10)
zip <- c('48501','48502', '48503', '48504','48505', '48506', '48507','48509')
BLLflint <- BLL %>% subset(ZCTA5CE10 %in% zip)
BLLflint[is.na(BLLflint)] <- 0

###Joining city data with BLL data based on the zipcode
BLLZIPCODE <- dplyr::left_join(BLLflint,FlintZP,  by ='ZCTA5CE10')
BLLZIPCODE <- st_as_sf(BLLZIPCODE)
BLLZIPCODE <- st_transform(BLLZIPCODE, crs = 26915, proj4string = "+proj=longlat +datum=WGS84")
SFBLLZIPCODE <- st_as_sf(BLLZIPCODE)
SFBLLZIPCODE <- st_transform(SFBLLZIPCODE, crs = 26915, proj4string = "+proj=longlat +datum=WGS84")
SFFLINTZP <- st_as_sf(FlintZP)
SFFLINTZP <- st_transform(SFFLINTZP, crs = 26915, proj4string = "+proj=longlat +datum=WGS84")

### Forming zipcode grid
zipcodeGrid <- st_make_grid(SFFLINTZP, cellsize = 500, square=FALSE)
zipcodeGrid  <- st_transform(zipcodeGrid , crs = 26915, proj4string = "+proj=longlat +datum=WGS84")

###Converting the zipcode to sf object
sfZipcodeGrid <- st_as_sf(zipcodeGrid)
sfZipcodeGrid <-  mutate(sfZipcodeGrid, ID=c(1:nrow(sfZipcodeGrid)))

### Interpolating areally to zipcode grid from BLLZipcode
ar_validate(source=BLLZIPCODE , target=sfZipcodeGrid, varList = "any_samples_percent", method="aw", verbose=TRUE)
finalResult <- aw_interpolate(sfZipcodeGrid, tid="ID", source = BLLZIPCODE, sid="ZCTA5CE10", weight = "sum", output = "sf", intensive = "any_samples_percent")

###Interpolating areally to flint from BLLZipcode
ar_validate(source=BLLZIPCODE , target=flint, varList = "any_samples_percent", method="aw", verbose=TRUE)
flintFinalResult <- aw_interpolate(flint,tid="OBJECTID", source = BLLZIPCODE, sid="ZCTA5CE10", weight = "sum", output = "sf", intensive = "any_samples_percent")

### Plotting the results
ggplot(flintFinalResult) + geom_sf(aes(fill=any_samples_percent)) + scale_fill_gradient(low="white", high = "red", limits=c(0,5))
sfFLINT <- st_as_sf(flint)
sfFLINT <- st_transform(sfFLINT, crs = 26915, proj4string = "+proj=longlat +datum=WGS84")

ggplot(finalResult) + geom_sf(aes(fill=any_samples_percent))  + geom_sf(data=flintFinalResult, aes(fill=any_samples_percent)) + scale_fill_gradient(low="white", high = "red", limits=c(0,5))
ggplot(finalResult) + geom_sf(aes(fill=any_samples_percent))   + scale_fill_gradient(low="white", high = "red", limits=c(0,5))
@

\section{Exploring Modifiable Areal Unit Problem with Krigging interpolation:}
Before we started krigging, we converted all our sf objects we are planning to use to spdf. Since, we dont have any have methods to perform krigging directly on sf objects. The process of kriging starts with creating variograms and covariance functions to analyze the statistical relationship with known data. Typical models we often use are Spherical, Exponential, Circular models. We also attempted to find the best variogram model for the BLL data. We tried with several models(Spherical, Circular and Exponential). We used a fit.variogram function in R to find the most suitable variogram for this dataset. We found out that the Spherical is the best model to use. When we performed krigging from BLLZIPCODE to flint city, we got the BLL of 2.371, which is smaller than the value we got when we performed interpolation. We performed krigging interpolation three times, with the same dataset BLLZPspdf, but with different target datasets (flintspdf, zipcodespdf(sfzipcodegrid spdf object) and BLLALLzipcodes). So, to perform krigging from BLLZIPCODE to Zipcodespdf, we repeated the specified above process and got the below the visualization, the lighter the blue, the more serious the children from that area are affected by the lead. We tried to krige from all of the Michigan’s zip codes to the flint city.
<<ExploringMAUPusingFlintwaterdatawithKrigging Interpolation>>=

###Krigging to flint zipcode
BLLZPspdf <- sf::as_Spatial( BLLZIPCODE)
vgm1 <- variogram(any_samples_percent~1, BLLZPspdf)
plot(vgm1)
m.fit <- fit.variogram(vgm1, vgm(1, "Sph")) #this
vm2 = gstat::vgm(1.02, "Sph", 1098) # or this
flintspdf <- sf::as_Spatial(flint)
gstat::krige0(any_samples_percent ~ 1, BLLZPspdf, flintspdf, m.fit) 
flintbllkrig <- ggplot(flintspdf, aes( x= long, y = lat, group = group, fill = 2.37056 ))+ geom_polygon() 
flintbllkrig

###Krigging to zipcodegrid from BLLZipcode
zipcodespdf <- sf::as_Spatial(sfZipcodeGrid )
vgm3 <- variogram(ID~1, zipcodespdf)
plot(vgm3)
me.fit <- fit.variogram(vgm3, vgm(1, "Exp"))
vm4 <- gstat::vgm(1.000125, "Exp", 3707.834)

hold <- gstat::krige0(any_samples_percent ~ 1, BLLZPspdf, zipcodespdf,  vm4) 
zipcodespdf@data$hold <- c(hold)
zipcodespdf.points = fortify(zipcodespdf, region="ID")
zipcodespdf.points$id <- as.numeric(zipcodespdf.points$id)
zipcodespdf.df = inner_join(zipcodespdf.points, zipcodespdf@data, by=c("id" = "ID"))

ggplot(zipcodespdf.df, aes( x= long, y = lat, group = group, fill=hold))+ geom_polygon( ) 
@


<<FittingthedataonallzipcodeotherthanflintwithKriggingInterpolation>>=

### Krigging all zipcodes in BLL  
HOLD <-BLL$ZCTA5CE10
FlintZP <- zipcode %>% subset(ZCTA5CE10 %in% HOLD)
FlintZP[is.na(FlintZP)] <- 0
BLLALL <- dplyr::left_join(BLL,FlintZP,  by ='ZCTA5CE10')
BLLALL$any_samples_percent[is.na(BLLALL$any_samples_percent)] <- 0
BLLALL <- st_as_sf(BLLALL)
BLLALL <- st_transform(BLLALL, crs = 26915, proj4string = "+proj=longlat +datum=WGS84")
NBLLALL <- subset( BLLALL, st_is_empty(BLLALL$geometry) == FALSE)
BLLALLspdf <- sf::as_Spatial( NBLLALL)

vgm5 <- variogram(any_samples_percent~1, BLLALLspdf)
plot(vgm5)
an.fit <- me.fit <- fit.variogram(vgm5, vgm(1, "Sph"))
vgm6 <- gstat::vgm(an.fit$psill, "Sph", an.fit$range)
values <- gstat::krige0(any_samples_percent ~ 1, BLLALLspdf, flintspdf,  vgm6) 
#zipcodespdf@data$values <- c(values)
#zipcodespdf.points = fortify(zipcodespdf, region="ID")
#zipcodespdf.points$id <- as.numeric(zipcodespdf.points$id)
#zipcodespdf.df = inner_join(zipcodespdf.points, zipcodespdf@data, by=c("id" = "ID"))

ggplot(flintspdf, aes( x= long, y = lat, group = group, fill=.623))+ geom_polygon( ) 
@

\end{document}
